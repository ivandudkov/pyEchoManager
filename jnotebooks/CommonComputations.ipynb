{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52a73ffe-b7d2-4f04-9134-acbfeaac24a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import scipy.interpolate as interp\n",
    "import scipy as sc\n",
    "from numpy import pi, cos, sin, log, exp\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "if not '..' in sys.path:\n",
    "    sys.path.append('..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0f78e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2023, 5, 14, 12, 2, 44)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.utcfromtimestamp(1684065764.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86f83246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2023, 5, 14, 14, 2, 44)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.utcfromtimestamp(1684072964.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f55eb92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69827.23235471999\n",
      "90443.84617901506\n",
      "\n",
      "\n",
      "1002434.9566821381\n",
      "1004081.7144593628\n"
     ]
    }
   ],
   "source": [
    "array1 = [47906.42060995102, 46995.42060995102, 19167.67854895592, 2201.768243980408]\n",
    "array2 = [47906.42060995102, 46995.42060995102, 19167.67854895592, 2201.768243980408, 57482.57939004898]\n",
    "print(np.linalg.norm(array1))\n",
    "print(np.linalg.norm(array2))\n",
    "print('\\n')\n",
    "\n",
    "array1 = [47906.42060995102, 46995.42060995102, 19167.67854895592, 2201.768243980408, 1000000]\n",
    "array2 = [47906.42060995102, 46995.42060995102, 19167.67854895592, 2201.768243980408, 57482.57939004898, 1000000]\n",
    "print(np.linalg.norm(array1))\n",
    "print(np.linalg.norm(array2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152412dc-1814-4461-8bce-897302d2ab1c",
   "metadata": {},
   "source": [
    "# Расчёт полосы съёмки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa7cfadd-73cc-48bf-8bae-fb3cd9e9faab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.31423960146778\n",
      "24.727296775091595\n"
     ]
    }
   ],
   "source": [
    "sector_coverage = 70*pi/180 # deg\n",
    "depth = 9 # m\n",
    "sv = 1475 # m/s\n",
    "\n",
    "SR = depth/cos(sector_coverage)\n",
    "print(SR)\n",
    "\n",
    "swath_length = np.sqrt(SR**2 - depth**2)\n",
    "print(swath_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe11d6d-66e7-4920-a3d8-82559c944968",
   "metadata": {},
   "source": [
    "# Parasound P-70 Metainfo files Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7efdc94-42c6-4c41-a7a2-374aff0269f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parasound P-70 Metainfo files Parsing\n",
    "# meta_paths = find_ext('.inf')\n",
    "# desired_path = meta_paths[1]\n",
    "\n",
    "# output_file = os.path.splitext(desired_path)[0] + '_delays' + '.txt'\n",
    "# print(output_file)\n",
    "\n",
    "# header = ['Datetime', 'Delay[m]']\n",
    "# delays_list = []\n",
    "# datetime_list = []\n",
    "# for path in meta_paths:\n",
    "#     output_file = os.path.splitext(path)[0] + '_delays' + '.txt'\n",
    "    \n",
    "#     with open(path) as f:\n",
    "#         content = f.read().splitlines()\n",
    "\n",
    "#         for line in content:\n",
    "#             line_content = line.split(',')\n",
    "\n",
    "#             if line_content[1] == header[1]:\n",
    "#                 datetime_list.append(line_content[0])\n",
    "#                 delays_list.append(line_content[2])\n",
    "\n",
    "#     with open(output_file, 'w') as f:\n",
    "#         f.write(header[0] + ' ' + header[1] + '\\n')\n",
    "\n",
    "#         for num, line in enumerate(datetime_list):\n",
    "#             f.write(datetime_list[num] + ' ' + delays_list[num] + '\\n')\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e42e10-b1e1-4d6c-9a5f-3b932e7d317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parasound P-70 Metainfo files Parsing\n",
    "# meta_paths = find_ext('.inf')\n",
    "# desired_path = meta_paths[1]\n",
    "\n",
    "# path = os.path.splitext(desired_path)[0] + '_delays' + '.txt'\n",
    "# print(output_file)\n",
    "\n",
    "# header = ['Datetime', 'Delay[m]']\n",
    "# delays_list = []\n",
    "# datetime_list = []\n",
    "\n",
    "# output_file = os.path.splitext(path)[0] + '_delays' + '.txt'\n",
    "# with open(path) as f:\n",
    "#     content = f.read().splitlines()\n",
    "\n",
    "#     for line in content:\n",
    "#         line_content = line.split(',')\n",
    "\n",
    "#         if line_content[1] == header[1]:\n",
    "#             datetime_list.append(line_content[0])\n",
    "#             delays_list.append(line_content[2])\n",
    "# with open(output_file, 'w') as f:\n",
    "#     f.write(header[0] + ' ' + header[1] + '\\n')\n",
    "\n",
    "#     for num, line in enumerate(datetime_list):\n",
    "#         f.write(datetime_list[num] + ' ' + delays_list[num] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe30fc9d-f271-4b86-925b-5c185c0730c1",
   "metadata": {},
   "source": [
    "# MiniSVP format parser.\n",
    "\n",
    "Скрипт для выгрузки из .TXT Valeport MiniSVP метаданных о времени записи файла, его имени и серийном номере зонда.\n",
    "\n",
    "Это необходимо для формирования таблицы SVP зондирований"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ebab9a8-f7e6-403f-b260-71a808b55239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mycode.find_files_with_extension import find_file_with_extension\n",
    "\n",
    "def write_miniSVP_metadata(SVP_path, SVP_list_path):\n",
    "    with open(SVP_path, 'r') as f:\n",
    "        file_content = f.read().splitlines()\n",
    "\n",
    "    date = file_content[0].split(' ')[1].split('/')\n",
    "    time = file_content[0].split(' ')[2].split(':')\n",
    "\n",
    "    datetime = date[2] + date[1] + date[0] + '-' + time[0] + time[1] + '00.000000'\n",
    "    serial_num = file_content[2].split(' ')[2]\n",
    "    filename = os.path.split(SVP_path)[-1].split('.')[0]\n",
    "\n",
    "    if os.path.exists(SVP_list_path):\n",
    "        with open(SVP_list_path, 'a') as f:\n",
    "            f.write(f\"{filename}\\t{serial_num}\\t{datetime}\\n\")\n",
    "    else:\n",
    "        with open(SVP_list_path, 'w') as f:\n",
    "            f.write('Fname\\tS/N\\tDatetime\\n')\n",
    "            f.write(f\"{filename}\\t{serial_num}\\t{datetime}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1ce0abc-34ae-4b0d-a0b7-bbe67037cfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching *.TXT files in directory:Z:\\ABP_50_TTR-21\\af_SVPData\\MiniSVP raw\n"
     ]
    }
   ],
   "source": [
    "SVP_paths = find_file_with_extension(extension='.TXT', path=r'Z:\\ABP_50_TTR-21\\af_SVPData\\MiniSVP raw')\n",
    "\n",
    "for path in SVP_paths:\n",
    "    write_miniSVP_metadata(path, r'Z:\\ABP_50_TTR-21\\af_SVPData\\MiniSVP raw\\SVP_metadata_file.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeafb8a-f103-4c0a-adc8-21e4926b1b45",
   "metadata": {},
   "source": [
    "# Преобразование csv Sound Speed Manager в .svp PDS2000\n",
    "\n",
    "Скрипт для конвертации обработанных в SoundSpeedManager .TXT данных Valeport MiniSVP в формат .svp для проекта PDS2000\n",
    "\n",
    "Для данного скрипта необходим журнал SVP зондирований, с шапкой: ['NumO', 'Num', 'S/N', 'FileName', 'Date', 'TimeUTC', 'DateTimeUTCOpCPN', 'DateTimeUTCProbe', 'Station', 'Location', 'latN', 'lonE']\n",
    "\n",
    "S/N - серийный номер зонда; FileName - Имя файла .TXT; DateTimeUTCOpCPN -  время станции зондирования в UTC (не время из шапки файла .TXT; Station - название/номер станции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d71779f-8675-4337-b057-cc3f27591826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mycode.find_files_with_extension import find_file_with_extension\n",
    "\n",
    "def read_my_svp_logbook(path):\n",
    "    sn_list = []\n",
    "    fn_list = []\n",
    "    dt_list = []\n",
    "    st_list = []\n",
    "\n",
    "    with open(path) as f:\n",
    "        file_content = f.read().splitlines()\n",
    "        header = 1\n",
    "        for line in file_content:\n",
    "            if header == 1:\n",
    "                pass\n",
    "            else:\n",
    "                line_content = line.split('\\t')\n",
    "                # header: ['NumO', 'Num', 'S/N', 'FileName', 'Date', 'TimeUTC', 'DateTimeUTCOpCPN', 'DateTimeUTCProbe', 'Station', 'Location', 'latN', 'lonE']\n",
    "                sn_list.append(line_content[2])\n",
    "                fn_list.append(line_content[3])\n",
    "                dt_list.append(line_content[6])\n",
    "                st_list.append(line_content[8])\n",
    "            header = 0\n",
    "        return sn_list, fn_list, dt_list, st_list\n",
    "\n",
    "def modify_output_file_name(serial_num, logpath, output_path, filetype='SSMminiSVP'):\n",
    "    # Read logbook\n",
    "    sn_list, fn_list, dt_list, st_list = read_my_svp_logbook(logpath)\n",
    "    # Get the file name\n",
    "    out_folder_path, out_fname_ext2_ext1 = os.path.split(output_path)\n",
    "    out_fname_ext2, out_ext1 = os.path.splitext(out_fname_ext2_ext1)\n",
    "    out_fname, ext2 = os.path.splitext(out_fname_ext2)\n",
    "\n",
    "    if filetype == 'SSMminiSVP':\n",
    "        fname = out_fname\n",
    "    elif filetype == 'Idronaut' or filetype == 'CTD90':\n",
    "        fname = out_fname_ext2\n",
    "    \n",
    "    # Find the file in the logbook\n",
    "    for lnum, fn in enumerate(fn_list):\n",
    "        if fn == fname:\n",
    "            if sn_list[lnum] == str(serial_num):\n",
    "                # Format: yyyy-mm-dd-HHMM\n",
    "                mod_time = f'{dt_list[lnum][0:4]}-{dt_list[lnum][4:6]}-{dt_list[lnum][6:8]}-{dt_list[lnum][8:]}'\n",
    "                mod_out_fname = f'{mod_time}_{st_list[lnum]}_{sn_list[lnum]}'\n",
    "                # modify output path\n",
    "                mod_fname_ext = mod_out_fname + '.svp'\n",
    "                mod_out_path = os.path.join(out_folder_path, mod_fname_ext)\n",
    "                \n",
    "                if mod_out_path is None:\n",
    "                    raise RuntimeError('Modified path is None. Something wrong with the code or the data')\n",
    "                return mod_out_path\n",
    "\n",
    "def convert_SMMcsv_to_PDSsvp(SSMcsv_path, PDSsvp_path):\n",
    "    # Step 1. Read Sound Speed Manager .csv file\n",
    "    pdepth = []\n",
    "    pSV = []\n",
    "    ptemp = []\n",
    "    psal = []\n",
    "\n",
    "    with open(SSMcsv_path, 'r') as f:\n",
    "        # field order: depth, sound speed, salinity, temperature\n",
    "        line_start = 5  # previous 5 lines are parts of a header\n",
    "        filecontent = f.read().splitlines()[line_start:]\n",
    "        for line in filecontent:\n",
    "            line_content = line.split(',')\n",
    "            pdepth.append(float(line_content[0]))\n",
    "            pSV.append(float(line_content[1]))\n",
    "            ptemp.append(float(line_content[3]))\n",
    "            psal.append(float(line_content[2]))\n",
    "\n",
    "    # Step 2. Create a new PDS2000 .svp and update it by SSM .csv values\n",
    "    pheader = '[POINTS]\\n'\n",
    "    pnum_end = len(pdepth)\n",
    "    offset_header = '\\n[OFFSET]\\n'\n",
    "    sv_offset_name = 'SVOffset'\n",
    "    sv_offset = '0'\n",
    "    sv_offset_line = '{:} = {:}\\n\\n'.format(sv_offset_name,\n",
    "                                        sv_offset)\n",
    "\n",
    "    # field order: depth, sound velocity, temperature, salinity\n",
    "    with open(PDSsvp_path, 'w') as f:\n",
    "        f.write(pheader)\n",
    "        for pnum in range(pnum_end):\n",
    "            # field order: depth, sound velocity, temperature, salinity\n",
    "            point = 'Point({:d}) = {:f},{:f},{:f},{:f}\\n'.format(pnum+1,\n",
    "                                                                 pdepth[pnum]*-1, \n",
    "                                                                 pSV[pnum], \n",
    "                                                                 ptemp[pnum], \n",
    "                                                                 psal[pnum])\n",
    "            f.write(point)\n",
    "        f.write(offset_header)\n",
    "        f.write(sv_offset_line)\n",
    "\n",
    "############################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c3bdc5-16dd-49fa-b920-501f3c93c6ec",
   "metadata": {},
   "source": [
    "serial_number_list - list с серийными номерами зондов\n",
    "\n",
    "data_folder_list1 - list с путями до папок с .csv данными (обработанными в SSM данными SVP зондов)\n",
    "\n",
    "my_svp_log_path - путь до журнала SVP зондирований"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ac74e71-4c9b-4d9b-945c-6a7a0cf860c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching *.csv files in directory:D:\\MyExpeditions\\Cr_20220601_ABP_049\\data_CTD_SVP\\Data\\SoundSpeedManager\\43946_proc\\To process\n",
      "Searching *.csv files in directory:D:\\MyExpeditions\\Cr_20220601_ABP_049\\data_CTD_SVP\\Data\\SoundSpeedManager\\75748_proc\\To process\n"
     ]
    }
   ],
   "source": [
    "# serial_number_list = [43946, 75748, 75749]\n",
    "serial_number_list = [43946, 75748]\n",
    "data_folder_list1 = ['D:\\\\MyExpeditions\\\\Cr_20220601_ABP_049\\\\data_CTD_SVP\\\\Data\\\\SoundSpeedManager\\\\43946_proc\\\\To process',\n",
    "                    'D:\\\\MyExpeditions\\\\Cr_20220601_ABP_049\\\\data_CTD_SVP\\\\Data\\\\SoundSpeedManager\\\\75748_proc\\\\To process']\n",
    "\n",
    "my_svp_log_path = 'D:\\\\MyExpeditions\\\\Cr_20220601_ABP_049\\\\data_CTD_SVP\\\\ABP049_SVP_ProcessingTable.txt'\n",
    "\n",
    "\n",
    "for sn, df in zip(serial_number_list, data_folder_list1):\n",
    "    SSM_csv_paths = find_file_with_extension('.csv', df)\n",
    "    \n",
    "    for SSM_csv_path in SSM_csv_paths:\n",
    "        PDS_svp_path = modify_output_file_name(sn, my_svp_log_path, SSM_csv_path)\n",
    "        convert_SMMcsv_to_PDSsvp(SSM_csv_path, PDS_svp_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061acfe6-54c7-4ede-8da5-5e7127a16068",
   "metadata": {},
   "source": [
    "# Преобразование .csv Idronaut от Андрея Коржа в .svp PDS2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee794768-0916-4b08-aafa-bb78d1f40eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_idronautcsv_to_PDSsvp(idronautcsv_path, PDSsvp_path):\n",
    "        # Step 1. Read Sound Speed Manager .csv file\n",
    "    pdepth = []\n",
    "    pSV = []\n",
    "    ptemp = []\n",
    "    psal = []\n",
    "\n",
    "    with open(idronautcsv_path, 'r') as f:\n",
    "        # field order: depth, sound speed, salinity, temperature\n",
    "        line_start = 1  # first line is a header\n",
    "        filecontent = f.read().splitlines()[line_start:]\n",
    "        for line in filecontent:\n",
    "            line_content = line.split(',')\n",
    "            pdepth.append(float(line_content[9]))\n",
    "            pSV.append(float(line_content[10]))\n",
    "            ptemp.append(float(line_content[2]))\n",
    "            psal.append(float(line_content[4]))\n",
    "\n",
    "    # Step 2. Create a new PDS2000 .svp and update it by SSM .csv values\n",
    "    pheader = '[POINTS]\\n'\n",
    "    pnum_end = len(pdepth)\n",
    "    offset_header = '\\n[OFFSET]\\n'\n",
    "    sv_offset_name = 'SVOffset'\n",
    "    sv_offset = '0'\n",
    "    sv_offset_line = '{:} = {:}\\n\\n'.format(sv_offset_name,\n",
    "                                        sv_offset)\n",
    "\n",
    "    # field order: depth, sound velocity, temperature, salinity\n",
    "    with open(PDSsvp_path, 'w') as f:\n",
    "        f.write(pheader)\n",
    "        for pnum in range(pnum_end):\n",
    "            # field order: depth, sound velocity, temperature, salinity\n",
    "            point = 'Point({:d}) = {:f},{:f},{:f},{:f}\\n'.format(pnum+1,\n",
    "                                                                 pdepth[pnum]*-1, \n",
    "                                                                 pSV[pnum], \n",
    "                                                                 ptemp[pnum], \n",
    "                                                                 psal[pnum])\n",
    "            f.write(point)\n",
    "        f.write(offset_header)\n",
    "        f.write(sv_offset_line)\n",
    "\n",
    "############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c36b96e-b7e4-4b81-9624-3d60776b99f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logpath = 'F:\\\\ABP_049\\\\Станции, логбуки и треки\\\\ABP049_Idronaut_Log.txt'\n",
    "datapath = 'F:\\\\ABP_049\\\\Станции, логбуки и треки\\\\CTD_Koleso\\\\CTD_Idronaut_OS316#494'\n",
    "\n",
    "sn = 'Idronaut'\n",
    "idronaut_csv_paths = find_file_with_extension('.csv', datapath)\n",
    "\n",
    "for idronaut_csv_path in idronaut_csv_paths:\n",
    "    PDS_svp_path = modify_output_file_name(sn, logpath, idronaut_csv_path, filetype='Idronaut')\n",
    "    convert_idronautcsv_to_PDSsvp(idronaut_csv_path, PDS_svp_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a17ec2-7f36-493a-82f6-4839e0dfdca9",
   "metadata": {},
   "source": [
    "# Преобразование .csv CTD90 от Виктора Кречика в PDS2000 .svp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7bcc1f5-5b3b-48d6-add3-007de010ac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ctd90csv_to_PDSsvp(idronautcsv_path, PDSsvp_path):\n",
    "        # Step 1. Read Sound Speed Manager .csv file\n",
    "    pdepth = []\n",
    "    pSV = []\n",
    "    ptemp = []\n",
    "    psal = []\n",
    "\n",
    "    with open(idronautcsv_path, 'r') as f:\n",
    "        # field order: depth, sound speed, salinity, temperature\n",
    "        line_start = 35  # Previous lines are a header\n",
    "        filecontent = f.read().splitlines()[line_start:]\n",
    "        for line in filecontent:\n",
    "            line_content = line.split(',')\n",
    "            depth = float(line_content[2])\n",
    "            sv = float(line_content[8].split(';')[0])\n",
    "            temp = float(line_content[3])\n",
    "            sal = float(line_content[4])\n",
    "            if depth <= 0:\n",
    "                pass\n",
    "            elif sal < 1:\n",
    "                pass\n",
    "            elif len(pdepth) > 1:\n",
    "                if len(pdepth) == 1:\n",
    "                    pass\n",
    "                elif pdepth[-1] >= depth and pdepth[-2] >= depth:\n",
    "                    pass\n",
    "                else:\n",
    "                    pdepth.append(depth)\n",
    "                    pSV.append(sv)\n",
    "                    ptemp.append(temp)\n",
    "                    psal.append(sal)\n",
    "            else:\n",
    "                pdepth.append(depth)\n",
    "                pSV.append(sv)\n",
    "                ptemp.append(temp)\n",
    "                psal.append(sal)\n",
    "\n",
    "    # Step 2. Create a new PDS2000 .svp and update it by SSM .csv values\n",
    "    pheader = '[POINTS]\\n'\n",
    "    pnum_end = len(pdepth)\n",
    "    offset_header = '\\n[OFFSET]\\n'\n",
    "    sv_offset_name = 'SVOffset'\n",
    "    sv_offset = '0'\n",
    "    sv_offset_line = '{:} = {:}\\n\\n'.format(sv_offset_name,\n",
    "                                        sv_offset)\n",
    "\n",
    "    # field order: depth, sound velocity, temperature, salinity\n",
    "    with open(PDSsvp_path, 'w') as f:\n",
    "        f.write(pheader)\n",
    "        for pnum in range(pnum_end):\n",
    "            # field order: depth, sound velocity, temperature, salinity\n",
    "            point = 'Point({:d}) = {:f},{:f},{:f},{:f}\\n'.format(pnum+1,\n",
    "                                                                 pdepth[pnum]*-1, \n",
    "                                                                 pSV[pnum], \n",
    "                                                                 ptemp[pnum], \n",
    "                                                                 psal[pnum])\n",
    "            f.write(point)\n",
    "        f.write(offset_header)\n",
    "        f.write(sv_offset_line)\n",
    "\n",
    "############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8073b04-ee0f-4769-9e01-b12d92088ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "logpath = 'F:\\\\ABP_049\\\\Станции, логбуки и треки\\\\ABP049_CTD90_Log.txt'\n",
    "datapath = 'F:\\\\ABP_049\\\\Станции, логбуки и треки\\\\CTD_Krechik\\\\CTD90'\n",
    "\n",
    "sn = 'CTD90'\n",
    "ctd90_csv_paths = find_file_with_extension('.CSV', datapath)\n",
    "\n",
    "for ctd90_csv_path in ctd90_csv_paths:\n",
    "    PDS_svp_path = modify_output_file_name(sn, logpath, ctd90_csv_path, filetype='CTD90')\n",
    "    convert_ctd90csv_to_PDSsvp(ctd90_csv_path, PDS_svp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30598c4f-754e-483b-9757-1056de065a2b",
   "metadata": {},
   "source": [
    "# Скрипт для разделения .csv PDS трека записей на множество .csv, индивидуальных для каждого файла, а затем объединение их в .csv треков файлсетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71328b41-907f-4e4b-8480-bc3421a718ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.interpolate as interp\n",
    "import scipy\n",
    "from numpy import pi, cos, sin, log, exp\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from mycode.find_files_with_extension import find_file_with_extension\n",
    "from mbesmanager.pds_fileset_class import PDSFileset\n",
    "from mbesmanager.pds_filetrack_class import PDSFileTrack, read_filetrack_csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1b4b985-89ec-4d91-986e-44c45b354761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pdsfile_track_csv(ftr, storage_path):\n",
    "    ftr_name = ftr.fname\n",
    "    path = os.path.join(storage_path, ftr_name+'.csv')\n",
    "    \n",
    "    with open(path, 'w') as f:\n",
    "        header = f'Project,Fname,Time,Projection,X,Y,SSV,Depth,Heading\\n'\n",
    "        f.write(header)\n",
    "        for index, time in enumerate(ftr.tr_time):\n",
    "            f.write(f'{ftr.project},{ftr.fname},{datetime.utcfromtimestamp(time).isoformat()},{ftr.projection},{ftr.tr_x[index]},{ftr.tr_y[index]},{ftr.tr_sv[index]},{ftr.tr_depth[index]},{ftr.tr_heading[index]}\\n')\n",
    "\n",
    "def create_fset_path_csv(fs_obj, ft_obj_list, storage_path):\n",
    "    fileset_name = os.path.splitext(fs_obj.fileset_name)[0]\n",
    "    path_st = os.path.join(storage_path, fileset_name + '.csv')\n",
    "\n",
    "    with open(path_st, 'w') as f:\n",
    "        header = f'Project,FSname,Fname,Time,Projection,X,Y,SSV,Depth,Heading\\n'\n",
    "        f.write(header)\n",
    "        for ft_obj in ft_obj_list:\n",
    "            if ft_obj.fname + ft_obj.fext in fs_obj.linked_pds_files_names:\n",
    "                for index, time in enumerate(ft_obj.tr_time):\n",
    "                    f.write(f'{ft_obj.project},{fileset_name},{ft_obj.fname},{datetime.utcfromtimestamp(time).isoformat()},{ft_obj.projection},{ft_obj.tr_x[index]},{ft_obj.tr_y[index]},{ft_obj.tr_sv[index]},{ft_obj.tr_depth[index]},{ft_obj.tr_heading[index]}\\n')\n",
    "                    \n",
    "def create_fsets_path_csv(fs_obj_list, ft_obj_list, output_csv_file_path):\n",
    "    with open(output_csv_file_path, 'w') as f:\n",
    "        header = f'Project,FSname,Fname,Time,Projection,X,Y,SSV,Depth,Heading\\n'\n",
    "        f.write(header)\n",
    "        for fs_obj in fs_obj_list:\n",
    "            fs_name = os.path.splitext(fs_obj.fileset_name)[0]\n",
    "            for ft_obj in ft_obj_list:\n",
    "                if ft_obj.fname + ft_obj.fext in fs_obj.linked_pds_files_names:\n",
    "                    for index, time in enumerate(ft_obj.tr_time):\n",
    "                        f.write(f'{ft_obj.project},{fs_name},{ft_obj.fname},{datetime.utcfromtimestamp(time).isoformat()},{ft_obj.projection},{ft_obj.tr_x[index]},{ft_obj.tr_y[index]},{ft_obj.tr_sv[index]},{ft_obj.tr_depth[index]},{ft_obj.tr_heading[index]}\\n')\n",
    "\n",
    "def create_fsets_path_csv_ts_te(fs_obj_list, ft_obj_list, output_csv_file_path, error_list=os.path.join(os.getcwd(), 'create_fsets_path_csv_ts_te_errors.txt')):\n",
    "    with open(output_csv_file_path, 'w') as f:\n",
    "        header = f'Project,FSname,Fname,Time,StOrEn,Projection,X,Y,SSV,Depth,Heading\\n'\n",
    "        f.write(header)\n",
    "        for fs_obj in fs_obj_list:\n",
    "            fs_name = os.path.splitext(fs_obj.fileset_name)[0]\n",
    "            for ft_obj in ft_obj_list:\n",
    "                if ft_obj.fname + ft_obj.fext in fs_obj.linked_pds_files_names:\n",
    "                    index_srt = 0\n",
    "                    index_end = -1\n",
    "                    try:\n",
    "                        ft_obj.tr_time[0]\n",
    "                    except:\n",
    "                        print(f\"File {ft_obj.fname} in fileset {fs_name} doesn't have start nor end values\")\n",
    "                        err_file = open(error_list, 'a')\n",
    "                        err_file.write(f'{datetime.utcnow().isoformat()} {ft_obj.fname}\\n')\n",
    "                        err_file.close()\n",
    "                                \n",
    "                    else:\n",
    "                        f.write(f'{ft_obj.project},{fs_name},{ft_obj.fname},{datetime.utcfromtimestamp(ft_obj.tr_time[0]).isoformat()},Start,{ft_obj.projection},{ft_obj.tr_x[0]},{ft_obj.tr_y[0]},{ft_obj.tr_sv[0]},{ft_obj.tr_depth[0]},{ft_obj.tr_heading[0]}\\n')\n",
    "                        f.write(f'{ft_obj.project},{fs_name},{ft_obj.fname},{datetime.utcfromtimestamp(ft_obj.tr_time[-1]).isoformat()},End,{ft_obj.projection},{ft_obj.tr_x[-1]},{ft_obj.tr_y[-1]},{ft_obj.tr_sv[-1]},{ft_obj.tr_depth[-1]},{ft_obj.tr_heading[-1]}\\n')\n",
    "                    \n",
    "def fileset_load_tr(fileset_paths_from):\n",
    "    filesets = list()\n",
    "    size = 0\n",
    "    file_number = 0\n",
    "    fileset_number = 0\n",
    "    for path in fileset_paths_from:\n",
    "        fileset = PDSFileset()\n",
    "    \n",
    "        # path - is a full path to the *.sub fileset\n",
    "        split_path = path.split(sep='\\\\')\n",
    "        fileset.fileset_name = split_path[-1]\n",
    "        fileset.fileset_path = path\n",
    "\n",
    "        with open(path) as f:\n",
    "            file_read = f.readlines()\n",
    "            for idx, line in enumerate(file_read):\n",
    "                line_content = line.splitlines()[0]\n",
    "                \n",
    "                # Declare the header\n",
    "                if idx == 0:\n",
    "                    fileset.header = line_content\n",
    "                # Cut the end\n",
    "                elif (line_content.rstrip('\\n')).rstrip(' ') == '':\n",
    "                    pass\n",
    "                # The content\n",
    "                else:\n",
    "                    pds_project_path = os.path.split(path)[0]\n",
    "                    split_line = line_content.split(sep=fileset.equal_symbol)\n",
    "                    fileset.file_num.append(split_line[0])\n",
    "                    split_path = split_line[1].split(sep='\\\\')\n",
    "                    fileset.relative_path_dir.append(split_path[0])\n",
    "                    fileset.relative_path_file.append(split_path[1])\n",
    "\n",
    "                    fileset.linked_pds_files_names.append(split_path[1] + '.pds')\n",
    "                    fileset.linked_pds_files_paths.append(os.path.join(pds_project_path,\n",
    "                                                                    fileset.relative_path_dir[-1],\n",
    "                                                                    fileset.linked_pds_files_names[-1]))\n",
    "\n",
    "                    fileset.linked_gpt_files_names.append(split_path[1] + '.gpt')\n",
    "                    fileset.linked_gpt_files_paths.append(os.path.join(pds_project_path,\n",
    "                                                                    fileset.relative_path_dir[-1],\n",
    "                                                                    fileset.linked_gpt_files_names[-1]))\n",
    "        filesets.append(fileset)\n",
    "    return filesets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dd5e107-0aff-4a72-969e-e78c2ee38e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching *.sub files in directory:Z:\\ABP_50_TTR-21\\zzzz_IvanDudkovFolder\\TrackProcessing\\filesets\n",
      "error at line 302626. depth: \n",
      "error at line 302627. depth: \n",
      "error at line 302628. depth: \n",
      "error at line 302633. depth: \n",
      "error at line 302634. depth: \n",
      "error at line 302635. depth: \n",
      "error at line 302636. depth: \n",
      "error at line 302637. depth: \n",
      "error at line 302638. depth: \n",
      "error at line 302639. depth: \n",
      "error at line 302640. depth: \n",
      "error at line 302641. depth: \n",
      "error at line 302642. depth: \n",
      "error at line 302643. depth: \n",
      "error at line 302644. depth: \n",
      "error at line 302645. depth: \n",
      "error at line 302646. depth: \n",
      "error at line 302647. depth: \n",
      "error at line 302648. depth: \n",
      "error at line 302649. depth: \n",
      "error at line 302650. depth: \n",
      "error at line 302651. depth: \n",
      "error at line 302652. depth: \n",
      "error at line 302653. depth: \n",
      "error at line 302654. depth: \n",
      "error at line 302655. depth: \n",
      "error at line 302656. depth: \n",
      "error at line 302657. depth: \n",
      "error at line 302658. depth: \n",
      "error at line 302659. depth: \n",
      "error at line 302660. depth: \n",
      "error at line 302661. depth: \n",
      "error at line 302662. depth: \n",
      "error at line 302663. depth: \n",
      "error at line 302664. depth: \n",
      "error at line 302665. depth: \n",
      "error at line 302666. depth: \n",
      "error at line 302667. depth: \n",
      "error at line 302668. depth: \n",
      "error at line 302669. depth: \n",
      "error at line 302670. depth: \n",
      "error at line 302671. depth: \n",
      "error at line 302672. depth: \n",
      "error at line 302673. depth: \n",
      "error at line 302674. depth: \n",
      "error at line 302675. depth: \n",
      "error at line 302676. depth: \n",
      "error at line 302678. depth: \n",
      "error at line 302679. depth: \n",
      "error at line 302681. depth: \n",
      "error at line 302682. depth: \n",
      "error at line 302683. depth: \n",
      "error at line 302684. depth: \n",
      "error at line 302685. depth: \n",
      "error at line 302687. depth: \n",
      "error at line 302688. depth: \n",
      "error at line 302690. depth: \n",
      "error at line 302691. depth: \n",
      "error at line 304546. depth: \n",
      "error at line 304547. depth: \n",
      "error at line 304548. depth: \n",
      "error at line 304549. depth: \n",
      "error at line 304550. depth: \n",
      "error at line 304551. depth: \n",
      "error at line 304552. depth: \n",
      "error at line 304553. depth: \n",
      "error at line 304554. depth: \n",
      "error at line 304555. depth: \n",
      "error at line 304556. depth: \n",
      "error at line 304557. depth: \n",
      "error at line 304558. depth: \n",
      "error at line 304559. depth: \n",
      "error at line 304560. depth: \n",
      "error at line 304561. depth: \n",
      "error at line 304562. depth: \n",
      "error at line 304563. depth: \n",
      "error at line 304564. depth: \n",
      "error at line 304565. depth: \n",
      "error at line 304566. depth: \n",
      "error at line 304567. depth: \n",
      "error at line 304568. depth: \n",
      "error at line 304569. depth: \n",
      "error at line 304570. depth: \n",
      "error at line 304571. depth: \n",
      "error at line 304572. depth: \n",
      "error at line 304573. depth: \n",
      "error at line 304574. depth: \n",
      "error at line 304575. depth: \n",
      "error at line 304576. depth: \n",
      "error at line 304577. depth: \n",
      "error at line 304578. depth: \n",
      "error at line 304579. depth: \n",
      "error at line 304580. depth: \n",
      "error at line 304581. depth: \n",
      "error at line 304582. depth: \n",
      "error at line 304583. depth: \n",
      "error at line 304584. depth: \n",
      "error at line 304585. depth: \n",
      "error at line 304586. depth: \n",
      "error at line 304587. depth: \n",
      "error at line 304588. depth: \n",
      "error at line 304589. depth: \n",
      "error at line 304590. depth: \n",
      "error at line 304591. depth: \n",
      "error at line 304592. depth: \n",
      "error at line 304593. depth: \n",
      "error at line 304594. depth: \n",
      "error at line 304595. depth: \n",
      "error at line 304596. depth: \n",
      "error at line 304597. depth: \n",
      "error at line 304598. depth: \n",
      "error at line 304600. depth: \n",
      "error at line 304601. depth: \n",
      "error at line 304603. depth: \n",
      "error at line 304604. depth: \n",
      "error at line 304605. depth: \n",
      "error at line 304606. depth: \n",
      "error at line 304607. depth: \n",
      "error at line 304609. depth: \n",
      "error at line 304610. depth: \n",
      "error at line 304612. depth: \n",
      "error at line 304613. depth: \n",
      "error at line 481689. depth: \n",
      "error at line 481690. depth: \n",
      "error at line 481691. depth: \n",
      "error at line 481692. depth: \n",
      "error at line 481698. depth: \n",
      "error at line 481704. depth: \n",
      "error at line 481705. depth: \n",
      "error at line 508673. depth: \n",
      "error at line 508674. depth: \n",
      "error at line 701313. depth: \n",
      "error at line 701314. depth: \n",
      "error at line 701315. depth: \n",
      "error at line 701316. depth: \n",
      "error at line 701317. depth: \n",
      "error at line 701318. depth: \n",
      "error at line 701319. depth: \n",
      "error at line 701320. depth: \n",
      "error at line 701321. depth: \n",
      "error at line 701322. depth: \n",
      "error at line 701323. depth: \n",
      "error at line 701324. depth: \n",
      "error at line 701325. depth: \n",
      "error at line 701326. depth: \n",
      "error at line 701327. depth: \n",
      "error at line 701328. depth: \n",
      "error at line 701329. depth: \n",
      "error at line 701330. depth: \n",
      "error at line 701331. depth: \n",
      "error at line 701332. depth: \n",
      "error at line 701333. depth: \n",
      "error at line 701334. depth: \n",
      "error at line 701335. depth: \n",
      "error at line 701336. depth: \n",
      "error at line 701337. depth: \n",
      "error at line 701338. depth: \n",
      "error at line 701339. depth: \n",
      "error at line 701340. depth: \n",
      "error at line 701341. depth: \n",
      "error at line 701342. depth: \n",
      "error at line 701343. depth: \n",
      "error at line 701344. depth: \n",
      "error at line 701345. depth: \n",
      "error at line 701346. depth: \n",
      "error at line 701381. depth: \n",
      "error at line 701382. depth: \n",
      "error at line 701386. depth: \n",
      "File ABP50_TTR021-20220730-031632-CL.01 in fileset fs_017_001_ClioneP2G4_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220730-043750-CL.01 in fileset fs_018_001_ClioneP2G5_Pr doesn't have start nor end values\n",
      "File ABP50_TTR021-20220730-092052-CL.05 in fileset fs_021_002_ClioneP2G8_Pr doesn't have start nor end values\n",
      "File ABP50_TTR021-20220730-092109-CL.05 in fileset fs_021_002_ClioneP2G8_Pr doesn't have start nor end values\n",
      "File ABP50_TTR021-20220730-092115-CL.05 in fileset fs_021_002_ClioneP2G8_Pr doesn't have start nor end values\n",
      "File ABP50_TTR021-20220730-092852-CL.05 in fileset fs_021_002_ClioneP2G8_Pr doesn't have start nor end values\n",
      "File ABP50_TTR021-20220730-092901-CL.05 in fileset fs_021_002_ClioneP2G8_Pr doesn't have start nor end values\n",
      "File ABP50_TTR021-20220730-093655-CL.09 in fileset fs_021_002_ClioneP2G8_Pr doesn't have start nor end values\n",
      "File ABP50_TTR021-20220730-114411-CL_AAA.11 in fileset fs_024_003_ClioneP2G11_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220730-114412-CL_AAA.11 in fileset fs_024_003_ClioneP2G11_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220730-153003-CL_AAA.11(3) in fileset fs_025_004_ClioneP2G15_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220730-170712-CL_AAA.11 in fileset fs_025_006_ClioneP2G17_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220730-170713-CL_AAA.11 in fileset fs_025_006_ClioneP2G17_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220730-171105-CL_AAA.11 in fileset fs_025_007_ClioneP2G18_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220730-171117-CL_BC1.1 in fileset fs_025_007_ClioneP2G18_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220730-192451-CL_BC4.1 in fileset fs_025_012_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220730-192504-CL_BD.01 in fileset fs_025_012_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220730-201445-CL_BE.01 in fileset fs_025_016_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220730-205936-CL_BE.01 in fileset fs_025_018_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220730-210731-CL_BE.01 in fileset fs_025_019_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220730-224916-CL_BD.07 in fileset fs_025_024_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220731-001132-CL_BD.04 in fileset fs_025_028_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220731-001328-CL_BD.04 in fileset fs_025_028_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220731-001541-CL_BD.04 in fileset fs_025_028_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220731-014044-CL_BD.03 in fileset fs_025_030_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220731-070350-CL_BG.1 in fileset fs_025_040_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220731-071605-CL_BG.1 in fileset fs_025_040_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220731-071924-CL_BG.1 in fileset fs_025_040_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220731-072804-CL_BG.1 in fileset fs_025_040_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220731-084211-CL_BY.1 in fileset fs_025_045_ClioneP2_Tr doesn't have start nor end values\n",
      "File ABP50_TTR021-20220731-090328-CL_add~line.0 in fileset fs_025_046_ClioneP2_St doesn't have start nor end values\n",
      "File ABP50_TTR021-20220731-093045-CL_add~line.0 in fileset fs_025_048_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220731-123221-CL_add~line.0 in fileset fs_025_057_ClioneP2_Tr doesn't have start nor end values\n",
      "File ABP50_TTR021-20220731-132119-CL_toSVP237.1 in fileset fs_025_060_ClioneP2_St doesn't have start nor end values\n",
      "File ABP50_TTR021-20220731-140624-CL_toSVP237.1 in fileset fs_025_062_ClioneP2_St doesn't have start nor end values\n",
      "File ABP50_TTR021-20220731-140626-CL_toSVP237.1 in fileset fs_025_062_ClioneP2_St doesn't have start nor end values\n",
      "File ABP50_TTR021-20220731-142203-CL_toSVP237.1 in fileset fs_025_062_ClioneP2_St doesn't have start nor end values\n",
      "File ABP50_TTR021-20220731-164403-CL_toSVP237.1 in fileset fs_025_068_ClioneP2_St doesn't have start nor end values\n",
      "File ABP50_TTR021-20220731-164535-CL_BE.01 in fileset fs_025_068_ClioneP2_St doesn't have start nor end values\n",
      "File ABP50_TTR021-20220731-165214-CL_BE.01 in fileset fs_025_068_ClioneP2_St doesn't have start nor end values\n",
      "File ABP50_TTR021-20220731-181243-CL_UL.01 in fileset fs_025_072_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220731-184856-CL_UL.01 in fileset fs_025_073_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220731-195946-CL_UL.01 in fileset fs_025_077_ClioneP2_Tr doesn't have start nor end values\n",
      "File ABP50_TTR021-20220731-195946-CL_ULp1.07(2) in fileset fs_025_077_ClioneP2_Tr doesn't have start nor end values\n",
      "File ABP50_TTR021-20220731-211414-CL_ULp1.06 in fileset fs_025_079_ClioneP2G22_Pr doesn't have start nor end values\n",
      "File ABP50_TTR021-20220731-212618-CL_ULp1.06 in fileset fs_025_080_ClioneP2G23_Pr doesn't have start nor end values\n",
      "File ABP50_TTR021-20220731-212635-CL_UL.01 in fileset fs_025_080_ClioneP2G23_Pr doesn't have start nor end values\n",
      "File ABP50_TTR021-20220731-220306-CL_UL.01 in fileset fs_025_080_ClioneP2G23_Pr doesn't have start nor end values\n",
      "File ABP50_TTR021-20220801-002604-CL_ULp1.03(2) in fileset fs_025_084_ClioneP2G26_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220801-002604-CL_ULp1.03 in fileset fs_025_084_ClioneP2G26_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220801-002858-CL_UL.0 in fileset fs_025_084_ClioneP2G26_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220801-002933-CL_UL.0 in fileset fs_025_084_ClioneP2G26_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220801-050027-CL_UL.01 in fileset fs_025_092_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220801-055518-CL_ULp2.14 in fileset fs_025_094_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220801-065204-CL_ULp1.03 in fileset fs_025_097_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220801-065332-CL_ULp1.03 in fileset fs_025_097_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220801-065407-CL_ULp1.03 in fileset fs_025_097_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220801-071221-CL_ULp1.03 in fileset fs_025_097_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220801-071248-CL_HH.1 in fileset fs_025_097_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220801-072603-CL_HH_2.1 in fileset fs_025_098_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220801-072913-CL_HH_2.1 in fileset fs_025_098_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220801-081339-CL_BE.01(3) in fileset fs_025_100_ClioneP2_St doesn't have start nor end values\n",
      "File ABP50_TTR021-20220801-081339-CL_BE.01(4) in fileset fs_025_100_ClioneP2_St doesn't have start nor end values\n",
      "File ABP50_TTR021-20220801-082252-CL_BE.01(4) in fileset fs_025_102_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220801-082252-CL_BE.01(8) in fileset fs_025_102_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220801-082304-CL_BE.01(3) in fileset fs_025_102_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220801-085324-CL_BF_A.04 in fileset fs_025_102_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220801-085349-CL_BE.01(2) in fileset fs_025_102_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220801-085349-CL_BE.01(4) in fileset fs_025_102_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220801-085410-CL_BE.01(3) in fileset fs_025_102_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220801-085410-CL_BE.01(4) in fileset fs_025_102_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220801-085410-CL_BE.01 in fileset fs_025_102_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220801-100454-CL_HH_2.1(3) in fileset fs_025_104_ClioneP2_Po doesn't have start nor end values\n",
      "File ABP50_TTR021-20220801-100454-CL_UL.01 in fileset fs_025_104_ClioneP2_Po doesn't have start nor end values\n"
     ]
    }
   ],
   "source": [
    "tracktable_path = 'Z:\\\\ABP_50_TTR-21\\\\zzzz_IvanDudkovFolder\\\\TrackProcessing\\\\track_spreadsheet\\\\VesselTrack_CLP2_v2.csv'\n",
    "filesets_path = 'Z:\\\\ABP_50_TTR-21\\\\zzzz_IvanDudkovFolder\\\\TrackProcessing\\\\filesets'\n",
    "######\n",
    "output_file_path = 'Z:\\\\ABP_50_TTR-21\\\\zzzz_IvanDudkovFolder\\\\TrackProcessing\\\\track_spreadsheet\\\\VesselTrack_CLP2_proc_v2.csv'\n",
    "output_srt_end_file_path = 'Z:\\\\ABP_50_TTR-21\\\\zzzz_IvanDudkovFolder\\\\TrackProcessing\\\\track_spreadsheet\\\\VesselTrack_CLP2_StEnd_v2.csv'\n",
    "error_list = 'Z:\\\\ABP_50_TTR-21\\\\zzzz_IvanDudkovFolder\\\\TrackProcessing\\\\track_spreadsheet\\\\VesselTrack_CLP2_ProcErrList_v2.csv'\n",
    "\n",
    "# tr_csv_storage_path = 'C:\\\\Workspace\\\\Projects\\\\Pr_MBESManager\\\\PDS2000_TrackProcessing\\\\pds_csv_tracks'\n",
    "# fs_csv_storage_path = 'C:\\\\Workspace\\\\Projects\\\\Pr_MBESManager\\\\PDS2000_TrackProcessing\\\\filesets_csv_tracks'\n",
    "\n",
    "fs_obj_paths = find_file_with_extension('.sub', filesets_path)\n",
    "fs_obj_list = fileset_load_tr(fs_obj_paths)\n",
    "ft_obj_list = read_filetrack_csv(tracktable_path, 'LambConfConic2Par')\n",
    "\n",
    "# Create one CSV file for all filesets and files. Only starts and ends of files:\n",
    "create_fsets_path_csv_ts_te(fs_obj_list, ft_obj_list, output_srt_end_file_path, error_list=error_list)\n",
    "\n",
    "# Create one CSV file for all filesets and files:\n",
    "create_fsets_path_csv(fs_obj_list, ft_obj_list, output_file_path)\n",
    "\n",
    "\n",
    "# Create CSV file for each .pds track:\n",
    "# for ft in ft_obj_list:\n",
    "#     create_pdsfile_track_csv(ft, tr_csv_storage_path)\n",
    "\n",
    "# Create CSV file for each .sub fileset:\n",
    "# for fs in fs_obj_list:\n",
    "#     create_fset_path_csv(fs, ft_obj_list, fs_csv_storage_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "542b0e3096725a2056735354746e8fb0274aaacae20b326a136954ba503430db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
